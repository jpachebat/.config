# Cluster-specific config (e.g. modules, aliases) goes here.

wqueue() {
    if [ "$1" = "all" ]; then
        watch -n 3 "
            echo '=== ALL GPU JOBS (cluster-wide) ===';
            squeue -p gpu,gpu_v100,gpu_a100,gpu_a100_80g -o '%8i %10P %15u %5D %8T %12M %20R %20b';
            echo '';
            echo '=== YOUR JOBS (squeue) ===';
            squeue -u \$USER;
            echo '';
            echo '=== TODAY HISTORY (sacct, newest first) ===';
            sacct -u \$USER --starttime today -X --format=JobID,Start,State,Elapsed,NodeList | sort -k2 -r
        "
    else
        watch -n 3 "
            echo '=== ACTIVE (squeue) ===';
            squeue -u \$USER;
            echo '';
            echo '=== TODAY HISTORY (sacct, newest first) ===';
            sacct -u \$USER --starttime today -X --format=JobID,Start,State,Elapsed,NodeList | sort -k2 -r
        "
    fi
}

# Watch nvidia-smi on a GPU node
#   wsmi         -> auto-pick first node where you have a job
#   wsmi node    -> e.g. wsmi cholesky-gpu03
wsmi() {
    local node

    if [ -n "$1" ]; then
        node="$1"
    else
        # take first non-null node where you have a job
        node=$(squeue -u "$USER" -o "%N" -h | grep -v "(null)" | head -n1)
    fi

    if [ -z "$node" ] || [ "$node" = "(null)" ]; then
        echo "No running jobs found for $USER."
        return 1
    fi

    echo "Connecting to $node..."
    ssh -t "$node" 'watch -n 2 "nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits"'
}
